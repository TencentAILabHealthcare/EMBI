{
    "name": "MHCBert_Pretrian",
    "n_gpu": 1,

    "dataset": {
        "type": "MAADataset",
        "args":{
            "seq_dir": "./data/processed_data/MHC_pseudo.csv",
            "token_length_list": "",
            "vocab_dir": "",
            "seed": 0,
            "seq_name": "MHC_pseudo_seq",
            "tokenizer_name": "common",
            "max_len": 40,
            "test_split": 0.01
        }
    },

    "model": {
        "bert": "bert",
        "args":{
            "hidden_size": 768,
            "num_hidden_layers": 12,
            "num_attention_heads": 12,
            "intermediate_size": 1536,
            "hidden_act": "gelu",
            "hidden_dropout_prob": 0.1,
            "attention_probs_dropout_prob": 0.1,
            "max_position_embeddings": 512,
            "type_vocab_size": 2,
            "initializer_range": 0.02,
            "position_embedding_type": "absolute"
        }
    },

    "metrics":{
        "blosum_dir": "./data/raw_data/blosum62.json",
        "blosum": true
    },

    "trainer": {
        "epochs": 25,
        "batch_size": 512,
        "save_dir": "../Result/",
        "lr": 5e-5,
        "warmup": 0.1,
        "eval_accumulation_steps": 1,
        "logging_steps": 100
    }

}
